{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fastdatascience/clinical_trial_risk/blob/fixes_nov_2022/train/ctgov/TrainPhaseAndArmAndNumSubjectsClassifier_05_Spacy_with_config.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install spacy-transformers"
      ],
      "metadata": {
        "id": "EpG7zWxUgCKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29998edd-7df3-4518-f4ec-c85214a0f28a"
      },
      "id": "EpG7zWxUgCKW",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy-transformers\n",
            "  Downloading spacy_transformers-1.1.8-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (1.12.1+cu113)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.8.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 17.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (2.4.5)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (3.4.2)\n",
            "Collecting transformers<4.22.0,>=3.4.0\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 87.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (4.1.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (8.1.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (0.6.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (1.10.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (1.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (3.0.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (4.64.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (0.4.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0.0,>=3.4.0->spacy-transformers) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.4.0->spacy-transformers) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (2022.9.24)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (0.7.9)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 85.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 24.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers) (6.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.4.0->spacy-transformers) (2.0.1)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, spacy-alignments, spacy-transformers\n",
            "Successfully installed huggingface-hub-0.10.1 spacy-alignments-0.8.6 spacy-transformers-1.1.8 tokenizers-0.12.1 transformers-4.21.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m spacy init fill-config base_config.cfg spacy_textcat_05.cfg"
      ],
      "metadata": {
        "id": "3t_pIMzNf0jN"
      },
      "id": "3t_pIMzNf0jN",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ad812236",
      "metadata": {
        "id": "ad812236"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import pickle as pkl\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# df_annotations = pd.read_csv(\"all_annotations.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ea7d33",
      "metadata": {
        "id": "e7ea7d33"
      },
      "source": [
        "# Get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "xCe1anFkGSrt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCe1anFkGSrt",
        "outputId": "c8eebca1-3ba3-4305-e7bf-0a49ee4f5f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "FdcTHlHsGWnk",
      "metadata": {
        "id": "FdcTHlHsGWnk"
      },
      "outputs": [],
      "source": [
        "df_annotations= pd.read_csv(\"/content/drive/MyDrive/data/filtered_for_phase_arms_subjects_02.csv.bz2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4048908c",
      "metadata": {
        "id": "4048908c"
      },
      "outputs": [],
      "source": [
        "#df_annotations= pd.read_csv(\"filtered_for_phase_arms_subjects_02.csv.bz2\")\n",
        "# df_annotations= pd.read_csv(\"/home/thomas/Downloads/filtered_for_phase_arms_subjects.csv.bz2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "wGilqqbTi76N",
      "metadata": {
        "id": "wGilqqbTi76N"
      },
      "outputs": [],
      "source": [
        "#df_annotations.text = df_annotations.text.apply(lambda t : t[:10000] if len(t) > 10000 else t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "mZ82V337HouW",
      "metadata": {
        "id": "mZ82V337HouW"
      },
      "outputs": [],
      "source": [
        "def get_num_subjects_clean(num):\n",
        "    if pd.isna(num):\n",
        "        return None\n",
        "    if num >= 10000:\n",
        "        return \"10000+\"\n",
        "    if num >= 1000:\n",
        "        return \"1000+\"\n",
        "    if num >= 500:\n",
        "        return \"500+\"\n",
        "    if num >= 200:\n",
        "        return \"200+\"\n",
        "    if num >= 100:\n",
        "        return \"100+\"\n",
        "    if num >= 50:\n",
        "        return \"50+\"\n",
        "    if num >= 25:\n",
        "        return \"25+\"\n",
        "    return \"1-24\"\n",
        "df_annotations[\"num_subjects_clean\"] = df_annotations[\"num_subjects\"].apply(get_num_subjects_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cKMRrJjQIF7U",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKMRrJjQIF7U",
        "outputId": "573e73d4-f15e-489f-8474-81f4c59e7f85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1-24      2980\n",
              "25+       2119\n",
              "50+       2016\n",
              "100+      1674\n",
              "200+      1656\n",
              "500+       816\n",
              "1000+      547\n",
              "10000+     112\n",
              "Name: num_subjects_clean, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df_annotations[\"num_subjects_clean\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "89e1c4f0",
      "metadata": {
        "id": "89e1c4f0"
      },
      "outputs": [],
      "source": [
        "# df_annotations = pd.read_csv(\"filtered_for_phase.csv.bz2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a12043ac",
      "metadata": {
        "id": "a12043ac"
      },
      "outputs": [],
      "source": [
        "# del file_to_pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bbe0ba77",
      "metadata": {
        "id": "bbe0ba77"
      },
      "outputs": [],
      "source": [
        "phase_map = {\"Phase 2\":\"2\",\n",
        "\"Phase 3\":\"3\",\n",
        "\"Phase 4\":\"4\",\n",
        "\"Phase 1\":\"1\",\n",
        "\"Phase 1/Phase 2\":\"1.5\",\n",
        "\"Not Applicable\":\"0\",\n",
        "\"Phase 2/Phase 3\":\"2.5\",\n",
        "\"Early Phase 1\":\"0.5\"}\n",
        "df_annotations[\"phase_clean\"] = df_annotations[\"phase\"].apply(lambda x : phase_map.get(x, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "DNjQhXOVrQxj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNjQhXOVrQxj",
        "outputId": "ee4a3b75-6b70-478e-a271-5a0b4ccc37e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0', '0.5', '1', '1.5', '2', '2.5', '3', '4']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "phase_clean_map = {}\n",
        "for idx, val in enumerate(sorted(set(phase_map.values()), key = lambda x : float(x))):\n",
        "  phase_clean_map[val] = idx\n",
        "# invert the dictionary\n",
        "phase_lookup = {v: k for k, v in phase_clean_map.items()}\n",
        "\n",
        "phase_list = [phase_lookup[x] for x in sorted(phase_lookup)]\n",
        "phase_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f99e999b",
      "metadata": {
        "id": "f99e999b"
      },
      "outputs": [],
      "source": [
        "def get_num_arms_clean(num):\n",
        "    if pd.isna(num):\n",
        "        return None\n",
        "    if num >= 5:\n",
        "        num = 5\n",
        "    return num\n",
        "df_annotations[\"num_arms_clean\"] = df_annotations[\"num_arms\"].apply(get_num_arms_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tq8Iseh1e_uQ",
      "metadata": {
        "id": "Tq8Iseh1e_uQ"
      },
      "source": [
        "# Begin Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0HHgTj8Qfub9",
      "metadata": {
        "id": "0HHgTj8Qfub9"
      },
      "outputs": [],
      "source": [
        "num_subjects_clean_map = {}\n",
        "for idx, val in enumerate(sorted(set(df_annotations[~df_annotations.num_subjects_clean.isna()].num_subjects_clean), key = lambda x : int(re.sub(r'\\D.*$', '', x)))):\n",
        "  num_subjects_clean_map[val] = idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6jQQHMvxkC2l",
      "metadata": {
        "id": "6jQQHMvxkC2l"
      },
      "outputs": [],
      "source": [
        "# invert the dictionary\n",
        "num_subjects_lookup = {v: k for k, v in num_subjects_clean_map.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "C6fm756mlUSk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6fm756mlUSk",
        "outputId": "55929821-2307-425d-c2e0-0a94aa65cda2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1-24', '25+', '50+', '100+', '200+', '500+', '1000+', '10000+']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "num_subjects_list = [num_subjects_lookup[x] for x in sorted(num_subjects_lookup)]\n",
        "num_subjects_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "sDIKJA57hR-K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDIKJA57hR-K",
        "outputId": "915ba4a3-1bd9-483b-d986-63049964516f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1-24': 0,\n",
              " '25+': 1,\n",
              " '50+': 2,\n",
              " '100+': 3,\n",
              " '200+': 4,\n",
              " '500+': 5,\n",
              " '1000+': 6,\n",
              " '10000+': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "num_subjects_clean_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "EFTomf_xfjTS",
      "metadata": {
        "id": "EFTomf_xfjTS"
      },
      "outputs": [],
      "source": [
        "def get_one_hot_num_subjects(x):\n",
        "  a = [0] * len(num_subjects_clean_map)\n",
        "  if x is None:\n",
        "    return a\n",
        "  a[num_subjects_clean_map[x]] = 1\n",
        "  return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "GCf8jZQZiBbC",
      "metadata": {
        "id": "GCf8jZQZiBbC"
      },
      "outputs": [],
      "source": [
        "df_annotations[\"num_subjects_one_hot\"] = df_annotations[\"num_subjects_clean\"].apply(get_one_hot_num_subjects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "RoXplcZ6q3N3",
      "metadata": {
        "id": "RoXplcZ6q3N3"
      },
      "outputs": [],
      "source": [
        "df_annotations[\"num_subjects_one_hot\"] = df_annotations[\"num_subjects_clean\"].apply(get_one_hot_num_subjects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "bGkQ4LeYq3QR",
      "metadata": {
        "id": "bGkQ4LeYq3QR"
      },
      "outputs": [],
      "source": [
        "def get_one_hot_num_arms(x):\n",
        "  a = [0] * 5\n",
        "  if x is not None and not pd.isna(x):\n",
        "    a[int(x - 1)] = 1\n",
        "  return a\n",
        "df_annotations[\"num_arms_one_hot\"] = df_annotations[\"num_arms_clean\"].apply(get_one_hot_num_arms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "POLLhzW_rFvz",
      "metadata": {
        "id": "POLLhzW_rFvz"
      },
      "outputs": [],
      "source": [
        "def get_one_hot_phase(x):\n",
        "  a = [0] * len(phase_clean_map)\n",
        "  if x is None:\n",
        "    return a\n",
        "  a[phase_clean_map[x]] = 1\n",
        "  return a\n",
        "df_annotations[\"phase_one_hot\"] = df_annotations[\"phase_clean\"].apply(get_one_hot_phase)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ondugbdEsLJ7",
      "metadata": {
        "id": "ondugbdEsLJ7"
      },
      "source": [
        "Concatenate the three bits of one-hot data into one column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "eykeDEeFr56g",
      "metadata": {
        "id": "eykeDEeFr56g"
      },
      "outputs": [],
      "source": [
        "concatenated_one_hot = []\n",
        "for i in range(len(df_annotations)):\n",
        "  concatenated = list(df_annotations.phase_one_hot.iloc[i]) + \\\n",
        "  list(df_annotations.num_arms_one_hot.iloc[i]) + \\\n",
        "  list(df_annotations.num_subjects_one_hot.iloc[i]) + [df_annotations.has_sap.iloc[i]]\n",
        "  concatenated_one_hot.append(concatenated)\n",
        "df_annotations[\"concatenated_one_hot\"] = concatenated_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "eFM7jbfutXfu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFM7jbfutXfu",
        "outputId": "416d6f6c-ce1e-4753-b1c6-4cc72220aa92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "np.sum(np.asarray([np.asarray(x) for x in df_annotations[\"concatenated_one_hot\"]]), axis=1).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "YmCgmZh3tmkn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmCgmZh3tmkn",
        "outputId": "7d7b6d45-6c5b-4424-976f-39ad4221a530"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 519.,  135., 1154.,  983., 4317.,  297., 2725., 1461., 3086.,\n",
              "       5228., 1456.,  908.,  913., 2980., 2119., 2016., 1674., 1656.,\n",
              "        816.,  547.,  112.,   nan])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "np.sum(np.asarray([np.asarray(x) for x in df_annotations[\"concatenated_one_hot\"]]), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "yuUe_QwxsQRE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuUe_QwxsQRE",
        "outputId": "b8837c7f-45b1-4942-9bb1-842d14cb8308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 22 classes in this multi-label classifier\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(concatenated)\n",
        "print (f\"There are {num_classes} classes in this multi-label classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "F_VhjeihirIu",
      "metadata": {
        "id": "F_VhjeihirIu"
      },
      "outputs": [],
      "source": [
        "df_train = df_annotations[df_annotations.train_val == \"train\"]\n",
        "df_val = df_annotations[df_annotations.train_val == \"val\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "pIWANTOGqDkg",
      "metadata": {
        "id": "pIWANTOGqDkg"
      },
      "outputs": [],
      "source": [
        "df_train_got_some_ground_truths = df_train[~df_train.num_subjects_clean.isna() | ~df_train.num_arms_clean.isna() | ~df_train.phase_clean.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "yfw08MW2sYTw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfw08MW2sYTw",
        "outputId": "32140f40-68ef-4ef2-d5a8-6f8360339b34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9535, 9538)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "len(df_train_got_some_ground_truths), len(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6da254a",
      "metadata": {
        "id": "e6da254a"
      },
      "source": [
        "# Begin Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "eb7993bd",
      "metadata": {
        "id": "eb7993bd"
      },
      "outputs": [],
      "source": [
        "# TRAINING_DATA = [\n",
        "#     [\"My little kitty is so special\", {\"KAT0\": True}],\n",
        "#     [\"Dude, Totally, Yeah, Video Games\", {\"KAT1\": True}],\n",
        "#     [\"Should I pay $1,000 for the iPhone X?\", {\"KAT1\": True}],\n",
        "#     [\"The iPhone 8 reviews are here\", {\"KAT1\": True}],\n",
        "#     [\"Noa is a great cat name.\", {\"KAT0\": True}],\n",
        "#     [\"We got a new kitten!\", {\"KAT0\": True}]\n",
        "# ]\n",
        "\n",
        "# TRAINING_DATA = []\n",
        "# for idx in range(len(df_train)):\n",
        "#     cats = {}\n",
        "#     for a in range(num_classes):\n",
        "#         cats[str(a)] = df_annotations[\"concatenated_one_hot\"].iloc[idx][a]\n",
        "\n",
        "#     text = df_train.text.iloc[idx]\n",
        "#     if len(text) > 1000000:\n",
        "#         text = text[:1000000]\n",
        "    \n",
        "#     TRAINING_DATA.append([text , cats])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess everything into two DocBins so that we can train from the command line with spaCy"
      ],
      "metadata": {
        "id": "vJX8GSp2k1jB"
      },
      "id": "vJX8GSp2k1jB"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "docs_train = []\n",
        "for i in range(10):\n",
        "  docs_train.append(DocBin())\n",
        "docs_val = DocBin()\n",
        "\n",
        "for idx in range(len(df_annotations)):\n",
        "  if idx % 100 == 0:\n",
        "    print (idx, len(df_annotations))\n",
        "  if df_annotations.train_val.iloc[idx] == \"train\":\n",
        "    docs_list = docs_train[idx % 10]\n",
        "  else:\n",
        "    docs_list = docs_val\n",
        "  text = str(df_annotations.text.iloc[idx])\n",
        "  if len(text) > 100000:\n",
        "      text = text[:100000]\n",
        "  doc = nlp(text)\n",
        "\n",
        "  cats = {}\n",
        "  for a in range(num_classes):\n",
        "        cats[str(a)] = df_annotations[\"concatenated_one_hot\"].iloc[idx][a]\n",
        "  doc.cats = cats\n",
        "\n",
        "  docs_list.add(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKHF4luDhdK1",
        "outputId": "06972baa-d1da-4f88-8633-b1ddec8d0fc0"
      },
      "id": "zKHF4luDhdK1",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 11926\n",
            "100 11926\n",
            "200 11926\n",
            "300 11926\n",
            "400 11926\n",
            "500 11926\n",
            "600 11926\n",
            "700 11926\n",
            "800 11926\n",
            "900 11926\n",
            "1000 11926\n",
            "1100 11926\n",
            "1200 11926\n",
            "1300 11926\n",
            "1400 11926\n",
            "1500 11926\n",
            "1600 11926\n",
            "1700 11926\n",
            "1800 11926\n",
            "1900 11926\n",
            "2000 11926\n",
            "2100 11926\n",
            "2200 11926\n",
            "2300 11926\n",
            "2400 11926\n",
            "2500 11926\n",
            "2600 11926\n",
            "2700 11926\n",
            "2800 11926\n",
            "2900 11926\n",
            "3000 11926\n",
            "3100 11926\n",
            "3200 11926\n",
            "3300 11926\n",
            "3400 11926\n",
            "3500 11926\n",
            "3600 11926\n",
            "3700 11926\n",
            "3800 11926\n",
            "3900 11926\n",
            "4000 11926\n",
            "4100 11926\n",
            "4200 11926\n",
            "4300 11926\n",
            "4400 11926\n",
            "4500 11926\n",
            "4600 11926\n",
            "4700 11926\n",
            "4800 11926\n",
            "4900 11926\n",
            "5000 11926\n",
            "5100 11926\n",
            "5200 11926\n",
            "5300 11926\n",
            "5400 11926\n",
            "5500 11926\n",
            "5600 11926\n",
            "5700 11926\n",
            "5800 11926\n",
            "5900 11926\n",
            "6000 11926\n",
            "6100 11926\n",
            "6200 11926\n",
            "6300 11926\n",
            "6400 11926\n",
            "6500 11926\n",
            "6600 11926\n",
            "6700 11926\n",
            "6800 11926\n",
            "6900 11926\n",
            "7000 11926\n",
            "7100 11926\n",
            "7200 11926\n",
            "7300 11926\n",
            "7400 11926\n",
            "7500 11926\n",
            "7600 11926\n",
            "7700 11926\n",
            "7800 11926\n",
            "7900 11926\n",
            "8000 11926\n",
            "8100 11926\n",
            "8200 11926\n",
            "8300 11926\n",
            "8400 11926\n",
            "8500 11926\n",
            "8600 11926\n",
            "8700 11926\n",
            "8800 11926\n",
            "8900 11926\n",
            "9000 11926\n",
            "9100 11926\n",
            "9200 11926\n",
            "9300 11926\n",
            "9400 11926\n",
            "9500 11926\n",
            "9600 11926\n",
            "9700 11926\n",
            "9800 11926\n",
            "9900 11926\n",
            "10000 11926\n",
            "10100 11926\n",
            "10200 11926\n",
            "10300 11926\n",
            "10400 11926\n",
            "10500 11926\n",
            "10600 11926\n",
            "10700 11926\n",
            "10800 11926\n",
            "10900 11926\n",
            "11000 11926\n",
            "11100 11926\n",
            "11200 11926\n",
            "11300 11926\n",
            "11400 11926\n",
            "11500 11926\n",
            "11600 11926\n",
            "11700 11926\n",
            "11800 11926\n",
            "11900 11926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# docs_train.to_disk(\"train.spacy\")\n",
        "docs_val.to_disk(\"dev.spacy\")"
      ],
      "metadata": {
        "id": "7LLBAyjkisjm"
      },
      "id": "7LLBAyjkisjm",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_val.to_disk(\"/content/drive/MyDrive/data/dev.spacy\")"
      ],
      "metadata": {
        "id": "uhVOW2-Sp70P"
      },
      "id": "uhVOW2-Sp70P",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir train.spacy\n",
        "# !mkdir /content/drive/MyDrive/data/train.spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbV7xnZip2Wj",
        "outputId": "c77d8feb-7132-4105-b334-0630bdadba88"
      },
      "id": "bbV7xnZip2Wj",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/data/train.spacy’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, doc_bin_train in enumerate(docs_train):\n",
        "  doc_bin_train.to_disk(f\"train.spacy/train_doc_bin{idx}.spacy\")\n",
        "  doc_bin_train.to_disk(f\"/content/drive/MyDrive/data/train.spacy/train_doc_bin{idx}.spacy\")"
      ],
      "metadata": {
        "id": "k3qf7_xEo_Dx"
      },
      "id": "k3qf7_xEo_Dx",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin training using spaCy CLI"
      ],
      "metadata": {
        "id": "h14ok40dsYqT"
      },
      "id": "h14ok40dsYqT"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train ./spacy_textcat_05.cfg --output /content/drive/MyDrive/data/output_textcat_05 --paths.train /content/drive/MyDrive/data/train.spacy --paths.dev /content/drive/MyDrive/data/dev.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PghYHYDVgktY",
        "outputId": "bac13e2b-c863-492d-ddf2-eaadcdf86424"
      },
      "id": "PghYHYDVgktY",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-14 15:41:38.716251: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/content/drive/MyDrive/data/output_textcat_05\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-11-14 15:41:45,958] [INFO] Set up nlp object from config\n",
            "INFO:spacy:Set up nlp object from config\n",
            "[2022-11-14 15:41:45,970] [INFO] Pipeline: ['transformer', 'textcat_multilabel']\n",
            "INFO:spacy:Pipeline: ['transformer', 'textcat_multilabel']\n",
            "[2022-11-14 15:41:45,975] [INFO] Created vocabulary\n",
            "INFO:spacy:Created vocabulary\n",
            "[2022-11-14 15:41:45,976] [INFO] Finished initializing nlp object\n",
            "INFO:spacy:Finished initializing nlp object\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "tcmalloc: large alloc 1073750016 bytes == 0x14fdce000 @  0x7f99904ca2a4 0x58ead6 0x441ff3 0x5d1f81 0x5d2306 0x58f62c 0x5105e2 0x58fd37 0x50ca37 0x58fd37 0x50ca37 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x7f9832e8f7c6 0x4ba72b 0x7f9832e8dce3 0x58f6e4 0x50ff13 0x5b575e 0x4bad0a 0x4d3249 0x591e56 0x50e18c 0x5b575e 0x58ff2e 0x50d482 0x5b575e 0x58ff2e\n",
            "[2022-11-14 15:49:26,172] [INFO] Initialized pipeline components: ['transformer', 'textcat_multilabel']\n",
            "INFO:spacy:Initialized pipeline components: ['transformer', 'textcat_multilabel']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'textcat_multilabel']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS TEXTC...  CATS_SCORE  SCORE \n",
            "---  ------  -------------  -------------  ----------  ------\n",
            "tcmalloc: large alloc 2147491840 bytes == 0x7f918fffa000 @  0x7f99904ca2a4 0x58ead6 0x441ff3 0x5d1f81 0x5d2306 0x58f62c 0x5105e2 0x58fd37 0x50ca37 0x58fd37 0x50ca37 0x4d01b4 0x50cb8d 0x4d01b4 0x50cb8d 0x4d01b4 0x5a04c4 0x5a2ed4 0x5909f6 0x510280 0x5b575e 0x58ff2e 0x50ca37 0x5b575e 0x58ff2e 0x50c4fc 0x4d01b4 0x50cb8d 0x5b575e 0x58ff2e 0x50d482\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "RuntimeError('CUDA out of memory. Tried to allocate 736.00 MiB (GPU 0; 39.59 GiB\n",
            "total capacity; 35.04 GiB already allocated; 2.19 MiB free; 38.09 GiB reserved\n",
            "in total by PyTorch) If reserved memory is >> allocated memory try setting\n",
            "max_split_size_mb to avoid fragmentation.  See documentation for Memory\n",
            "Management and PYTORCH_CUDA_ALLOC_CONF')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 532, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 75, in train\n",
            "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 122, in train\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 105, in train\n",
            "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 226, in train_while_improving\n",
            "    score, other_scores = evaluate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 281, in evaluate\n",
            "    scores = nlp.evaluate(dev_corpus(nlp))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1430, in evaluate\n",
            "    for eg, doc in zip(examples, docs):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1589, in pipe\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/util.py\", line 1651, in _pipe\n",
            "    yield from proc.pipe(docs, **kwargs)\n",
            "  File \"spacy/pipeline/trainable_pipe.pyx\", line 73, in pipe\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/util.py\", line 1600, in minibatch\n",
            "    batch = list(itertools.islice(items, int(batch_size)))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/util.py\", line 1651, in _pipe\n",
            "    yield from proc.pipe(docs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy_transformers/pipeline_component.py\", line 212, in pipe\n",
            "    self.set_annotations(subbatch, self.predict(subbatch))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy_transformers/pipeline_component.py\", line 228, in predict\n",
            "    activations = self.model.predict(docs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/model.py\", line 315, in predict\n",
            "    return self._func(self, X, is_train=False)[0]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy_transformers/layers/transformer_model.py\", line 185, in forward\n",
            "    model_output, bp_tensors = transformer(wordpieces, is_train)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/model.py\", line 291, in __call__\n",
            "    return self._func(self, X, is_train=is_train)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/layers/pytorchwrapper.py\", line 143, in forward\n",
            "    Ytorch, torch_backprop = model.shims[0](Xtorch, is_train)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/shims/pytorch.py\", line 72, in __call__\n",
            "    return self.predict(inputs), lambda a: ...\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/shims/pytorch.py\", line 90, in predict\n",
            "    outputs = self._model(*inputs.args, **inputs.kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 858, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 531, in forward\n",
            "    output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 414, in forward\n",
            "    past_key_value=self_attn_past_key_value,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 343, in forward\n",
            "    output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 256, in forward\n",
            "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 736.00 MiB (GPU 0; 39.59 GiB total capacity; 35.04 GiB already allocated; 2.19 MiB free; 38.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"/content/drive/MyDrive/data/output_textcat_05\")"
      ],
      "metadata": {
        "id": "mDLgJUS-jCCd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "ebaa8098-d9c1-4c8c-c951-f76f0c1d2fdb"
      },
      "id": "mDLgJUS-jCCd",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-2ff73235ac06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/data/output_textcat_05\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89103b7c",
      "metadata": {
        "id": "89103b7c"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "for idx in range(len(df_val)):\n",
        "    doc = nlp(df_val.text.apply(str).iloc[idx])\n",
        "    predictions.append(doc.cats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4832e5ba",
      "metadata": {
        "id": "4832e5ba"
      },
      "outputs": [],
      "source": [
        "pred_proba = []\n",
        "for idx in range(len(df_val)):\n",
        "    pred_proba.append([predictions[idx][a] for a in range(num_classes)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wia77Kufub2v",
      "metadata": {
        "id": "wia77Kufub2v"
      },
      "outputs": [],
      "source": [
        "y_pred_phase = []\n",
        "y_pred_num_arms = []\n",
        "y_pred_num_subjects = []\n",
        "y_pred_sap = []\n",
        "for idx in range(len(pred_proba)):\n",
        "  probas_this_instance = pred_proba[idx]\n",
        "  probas_phase = probas_this_instance[:len(phase_lookup)]\n",
        "  y_pred_phase.append(phase_lookup[int(np.argmax(probas_phase))])\n",
        "  probas_arms = probas_this_instance[len(phase_lookup):len(phase_lookup)+5]\n",
        "  y_pred_num_arms.append(1 + int(np.argmax(probas_arms)))\n",
        "  probas_subjects = probas_this_instance[len(phase_lookup)+5:-1]\n",
        "  y_pred_num_subjects.append(num_subjects_lookup[int(np.argmax(probas_subjects))])\n",
        "  probas_sap = probas_this_instance[-1:]\n",
        "  y_pred_sap.append(probas_sap[0] > 0.5)\n",
        "\n",
        "df_val[\"y_pred_phase\"] = y_pred_phase\n",
        "df_val[\"y_pred_num_arms\"] = y_pred_num_arms\n",
        "df_val[\"y_pred_num_subjects\"] = y_pred_num_subjects\n",
        "df_val[\"y_pred_sap\"] = y_pred_sap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DbBjX9-agkVH",
      "metadata": {
        "id": "DbBjX9-agkVH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ZNUbeGPw5KWg",
      "metadata": {
        "id": "ZNUbeGPw5KWg"
      },
      "source": [
        "## Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sznpf_e24NvP",
      "metadata": {
        "id": "sznpf_e24NvP"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(df_val.phase_clean.apply(str), df_val[\"y_pred_phase\"])\n",
        "print (f\"Phase accuracy {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zLJ_MQih4T9n",
      "metadata": {
        "id": "zLJ_MQih4T9n"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(df_val.phase_clean.apply(str), df_val[\"y_pred_phase\"])\n",
        "plt.xticks(rotation=90)\n",
        ";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fz_EyOrB5PXa",
      "metadata": {
        "id": "Fz_EyOrB5PXa"
      },
      "source": [
        "# Number of arms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X8IsVDo34YTg",
      "metadata": {
        "id": "X8IsVDo34YTg"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(df_val.num_arms_clean.apply(float).apply(str), df_val[\"y_pred_num_arms\"].apply(float).apply(str))\n",
        "print (f\"Num arms accuracy {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "idb1W6VE4kIp",
      "metadata": {
        "id": "idb1W6VE4kIp"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(df_val.num_arms_clean.apply(float).apply(str), df_val[\"y_pred_num_arms\"].apply(float).apply(str))\n",
        "plt.xticks(rotation=90)\n",
        ";"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lLWC7P1k5NSX",
      "metadata": {
        "id": "lLWC7P1k5NSX"
      },
      "source": [
        "## Subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rYhLhDmnNuAl",
      "metadata": {
        "id": "rYhLhDmnNuAl"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(df_val.num_subjects_clean, df_val[\"y_pred_num_subjects\"])\n",
        "print (f\"Subjects accuracy {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XOiQVXB0NuHC",
      "metadata": {
        "id": "XOiQVXB0NuHC"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(df_val.num_subjects_clean, df_val[\"y_pred_num_subjects\"], labels=num_subjects_list)\n",
        "plt.xticks(rotation=90)\n",
        ";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VzFHDCRtmBA6",
      "metadata": {
        "id": "VzFHDCRtmBA6"
      },
      "outputs": [],
      "source": [
        "num_correct = 0\n",
        "for idx in range(len(df_val)):\n",
        "  gt = num_subjects_clean_map[df_val[\"num_subjects_clean\"].iloc[idx]]\n",
        "  pred = num_subjects_clean_map[df_val[\"y_pred_num_subjects\"].iloc[idx]]\n",
        "  is_correct = int(np.abs(gt - pred) <= 1)\n",
        "  num_correct += is_correct\n",
        "print (\"Accuracy including adjacent groups\", num_correct/len(df_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1MU1NT4T_DLO",
      "metadata": {
        "id": "1MU1NT4T_DLO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "tNsHUs9VBl4u",
      "metadata": {
        "id": "tNsHUs9VBl4u"
      },
      "source": [
        "## SAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w9KjSCypBmgs",
      "metadata": {
        "id": "w9KjSCypBmgs"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(df_val.has_sap, df_val[\"y_pred_sap\"])\n",
        "print (f\"SAP accuracy {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yt36Zw4wB4P9",
      "metadata": {
        "id": "Yt36Zw4wB4P9"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(df_val.has_sap, df_val[\"y_pred_sap\"])\n",
        "plt.xticks(rotation=90)\n",
        ";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f4a3b1",
      "metadata": {
        "id": "f5f4a3b1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d94c76",
      "metadata": {
        "id": "96d94c76"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd10b723",
      "metadata": {
        "id": "dd10b723"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python [conda env:py310] *",
      "language": "python",
      "name": "conda-env-py310-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}